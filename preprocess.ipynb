{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/z0g00mx/desktop/Intern_project\n",
      "Python 3.7.3\n",
      "README.md         als_pyspark.ipynb \u001b[34monboarding\u001b[m\u001b[m        query.sh\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m       bpr.ipynb         \u001b[34mpapers\u001b[m\u001b[m            \u001b[34mreport\u001b[m\u001b[m\n",
      "als.ipynb         \u001b[34mexperiment_data\u001b[m\u001b[m   preprocess.ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!python --version\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the distribution of training/validation/test data\n",
    "\n",
    "`train.txt`:\n",
    "    * ‘2019-03-25’ ~ ‘2019-05-19’, 2 months\n",
    "    * Number of transactions: 47938318\n",
    "    * Size of textfile: 5.1G\n",
    "`validation.txt`:\n",
    "    * ‘2019-05-20’ ~ ‘2019-06-02’, 2 weeks\n",
    "    * Number of transactions: 11546420\n",
    "    * Size of textfile: 1.3G\n",
    "`test.txt`:\n",
    "    * ‘2019-06-03’ to ‘2019-06-09’, 1 week\n",
    "    * Number of transactions: 6149672\n",
    "    * Size of textfile: 666M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get (cid, catalog_item_id) rows from train data\n",
    "file_path = \"../train.txt\"\n",
    "\n",
    "table = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split()\n",
    "        user, item = line[0], line[4]\n",
    "        table.append(np.asarray([user, item]))\n",
    "\n",
    "table = np.asarray(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14929995, 3008951)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing\n",
    "# cid -> user_id (0~N)\n",
    "# catalog_item_id -> item_id (0~M)\n",
    "user_map, item_map = {}, {}\n",
    "user_id = item_id = 0\n",
    "\n",
    "for row in table:\n",
    "    user, item = row[0], row[-1]\n",
    "    if user not in user_map:\n",
    "        user_map[user] = user_id\n",
    "        user_id += 1\n",
    "    if item not in item_map:\n",
    "        item_map[item] = item_id\n",
    "        item_id += 1\n",
    "\n",
    "N, M = len(user_map), len(item_map)\n",
    "N, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequencies about users buying and items bought\n",
    "user_dict = defaultdict(int)\n",
    "item_dict = defaultdict(int)\n",
    "\n",
    "for row in table:\n",
    "    user_id, item_id = user_map[row[0]], item_map[row[-1]]\n",
    "\n",
    "    user_dict[user_id] += 1\n",
    "    item_dict[item_id] += 1\n",
    "\n",
    "user_count = Counter(user_dict.values())\n",
    "item_count = Counter(item_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of dataset\n",
    "# \"customer_buy_frequency.csv\"\n",
    "sum_items = sum(user_count.values())\n",
    "\n",
    "# with open('./experiment_data/customer_buy_frequency.csv', 'w') as g:\n",
    "#     g.write(\"Frequency,# of Customers,%\\n\")\n",
    "#     for k, v in sorted(user_count.items()):\n",
    "#         g.write(\"{},{},{:.7f}\\n\".format(k, v, v/sum_items))\n",
    "\n",
    "# \"item_bought_frequency.csv\"\n",
    "sum_users = sum(item_count.values())\n",
    "\n",
    "# with open('./experiment_data/item_bought_frequency.csv', 'w') as h:\n",
    "#     h.write(\"Frequency,# of Items,%\\n\")\n",
    "#     for k, v in sorted(item_count.items()):\n",
    "#         h.write(\"{},{},{:.7f}\\n\".format(k, v, v/sum_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, set the threshold to avoid outliers like `infrequent customers/items, resellers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14895325, 2958922)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop infrequent user/item and resellers\n",
    "# determine the thresholds based on the distribution\n",
    "lower_user_threshold, upper_user_threshold = 45, 250\n",
    "lower_item_threshold = 150\n",
    "\n",
    "ignored_user_set, ignored_item_set = set(), set()\n",
    "\n",
    "for u, v in user_dict.items():\n",
    "    if v <= lower_user_threshold or v >= upper_user_threshold:\n",
    "        ignored_user_set.add(u)\n",
    "\n",
    "for i, v in item_dict.items():\n",
    "    if v <= lower_item_threshold:\n",
    "        ignored_item_set.add(i)\n",
    "\n",
    "len(ignored_user_set), len(ignored_item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34619, 47880)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-indexing after shrinking the data\n",
    "train_user_map, train_item_map = {}, {}\n",
    "new_u_id = new_i_id = 0\n",
    "\n",
    "for row in table:\n",
    "    user, item = row[0], row[-1]\n",
    "    if user_map[user] not in ignored_user_set and item_map[item] not in ignored_item_set:\n",
    "        if user not in train_user_map:\n",
    "            train_user_map[user] = new_u_id\n",
    "            new_u_id += 1\n",
    "        if item not in train_item_map:\n",
    "            train_item_map[item] = new_i_id\n",
    "            new_i_id += 1\n",
    "\n",
    "## fix bug: indices of user_map/item_map should be continuous\n",
    "# for u in user_map:\n",
    "#     if user_map[u] not in ignored_user_set:\n",
    "#         train_user_map[u] = new_u_id\n",
    "#         new_u_id += 1\n",
    "# for i in item_map:\n",
    "#     if item_map[i] not in ignored_item_set:\n",
    "#         train_item_map[i] = new_i_id\n",
    "#         new_i_id += 1\n",
    "        \n",
    "N, M = len(train_user_map), len(train_item_map)\n",
    "N, M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data table into dictionary, mapping as `user_id -> item_id -> value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 0/1 user-item matrix\n",
    "user_item_dict = defaultdict(lambda : defaultdict(float))\n",
    "# user_item_dict = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "for row in table:\n",
    "    user, item = row[0], row[-1]\n",
    "    if user in train_user_map and item in train_item_map:\n",
    "        user_id, item_id = train_user_map[user], train_item_map[item]\n",
    "        user_item_dict[user_id][item_id] += 1\n",
    "        # user_item_dict[user_id][item_id] = 1\n",
    "\n",
    "# if normalize (guess non-normalized data performs better)\n",
    "for uid in user_item_dict:\n",
    "    tmp = sum(user_item_dict[uid].values())\n",
    "    for iid in user_item_dict[uid]:\n",
    "        user_item_dict[uid][iid] /= tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix interaction:  1180452\n",
      "matrix density: 0.0712%\n"
     ]
    }
   ],
   "source": [
    "## calculate density of matrix\n",
    "# matrix = np.zeros((N, M))\n",
    "# for u in range(N):\n",
    "#     for i in range(M):\n",
    "#         matrix[u][i] = user_item_dict[u][i]\n",
    "# matrix_size = np.prod(matrix.shape)\n",
    "# interaction = np.flatnonzero(matrix).shape[0]\n",
    "\n",
    "matrix_size = N * M\n",
    "interaction = sum(len(user_item_dict[u].keys()) for u in user_item_dict)\n",
    "density = 100 * interaction / matrix_size\n",
    "\n",
    "print('matrix interaction: ', interaction)\n",
    "print('matrix density: {:.4f}%'.format(density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1180452 ../processed_train.txt\r\n"
     ]
    }
   ],
   "source": [
    "# write textfile\n",
    "# with open('../processed_train.txt', 'w') as m:\n",
    "# with open('../processed_train_2.txt', 'w') as m:\n",
    "#     for uid in sorted(user_item_dict):\n",
    "#         for iid in sorted(user_item_dict[uid]):\n",
    "#             m.write(\"{}\\t{}\\t{}\\n\".format(uid, iid, user_item_dict[uid][iid]))\n",
    "\n",
    "!wc -l ../processed_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_user_map' (dict)\n",
      "Stored 'train_item_map' (dict)\n"
     ]
    }
   ],
   "source": [
    "# store the mappings for users-based validation and test\n",
    "%store train_user_map  # from cid to user_id\n",
    "%store train_item_map  # from catalog_item_id to item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'cid_map' (dict)\n",
      "Stored 'catalog_item_id_map' (dict)\n"
     ]
    }
   ],
   "source": [
    "# get and store inverse mappings \n",
    "# user_id -> cid\n",
    "# item_id -> catalog_item_id\n",
    "cid_map = {v:k for k,v in train_user_map.items()}\n",
    "catalog_item_id_map = {v:k for k,v in train_item_map.items()}\n",
    "\n",
    "%store cid_map\n",
    "%store catalog_item_id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation size: 25048 users, 34208 items\n",
      "validation density: 0.0274%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234524"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess validation data\n",
    "file_path = '../validation.txt'\n",
    "\n",
    "validation_dict = defaultdict(lambda : defaultdict(int))\n",
    "eval_users, eval_items = set(), set()\n",
    "\n",
    "with open(file_path, 'r') as v:\n",
    "    for line in v.readlines():\n",
    "        line = line.split()\n",
    "        user, item = line[0], line[4]\n",
    "        if user in train_user_map and item in train_item_map:\n",
    "            user_id, item_id = train_user_map[user], train_item_map[item]\n",
    "            eval_users.add(user_id)\n",
    "            eval_items.add(item_id)\n",
    "            # validation_dict[user_id][item_id] = 1\n",
    "            validation_dict[user_id][item_id] += 1\n",
    "\n",
    "# if normalize to 0~1\n",
    "for uid in validation_dict:\n",
    "    tmp = sum(validation_dict[uid].values())\n",
    "    for iid in validation_dict[uid]:\n",
    "        validation_dict[uid][iid] /= tmp\n",
    "\n",
    "v_total = sum(len(validation_dict[u].keys()) for u in validation_dict)\n",
    "v_users, v_items = len(eval_users), len(eval_items)\n",
    "v_density = 100 * v_total / (v_users * v_items)\n",
    "\n",
    "print('validation size: {} users, {} items'.format(v_users, v_items))\n",
    "print('validation density: {:.4f}%'.format(v_density))\n",
    "\n",
    "# write validation file\n",
    "# with open('../processed_validation.txt', 'w') as m:\n",
    "# with open('../processed_validation_2.txt', 'w') as m:\n",
    "#     for uid in sorted(validation_dict):\n",
    "#         for iid in sorted(validation_dict[uid]):\n",
    "#             m.write(\"{}\\t{}\\t{}\\n\".format(uid, iid, validation_dict[uid][iid]))\n",
    "\n",
    "v_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 18612 users, 27496 items\n",
      "test density: 0.0246%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125816"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess test data\n",
    "file_path = '../test.txt'\n",
    "\n",
    "test_dict = defaultdict(lambda : defaultdict(int))\n",
    "test_users, test_items = set(), set()\n",
    "\n",
    "with open(file_path, 'r') as v:\n",
    "    for line in v.readlines():\n",
    "        line = line.split()\n",
    "        user, item = line[0], line[4]\n",
    "        if user in train_user_map and item in train_item_map:\n",
    "            user_id, item_id = train_user_map[user], train_item_map[item]\n",
    "            test_users.add(user_id)\n",
    "            test_items.add(item_id)\n",
    "            # test_dict[user_id][item_id] = 1\n",
    "            test_dict[user_id][item_id] += 1\n",
    "\n",
    "# if normalize to 0~1\n",
    "for uid in test_dict:\n",
    "    tmp = sum(test_dict[uid].values())\n",
    "    for iid in test_dict[uid]:\n",
    "        test_dict[uid][iid] /= tmp\n",
    "\n",
    "t_total = sum(len(test_dict[u].keys()) for u in test_dict)\n",
    "t_users, t_items = len(test_users), len(test_items)\n",
    "t_density = 100 * t_total / (t_users * t_items)\n",
    "\n",
    "print('test size: {} users, {} items'.format(t_users, t_items))\n",
    "print('test density: {:.4f}%'.format(t_density))\n",
    "\n",
    "# write validation file\n",
    "# with open('../processed_test.txt', 'w') as m:\n",
    "# with open('../processed_test_2.txt', 'w') as m:\n",
    "#     for uid in sorted(test_dict):\n",
    "#         for iid in sorted(test_dict[uid]):\n",
    "#             m.write(\"{}\\t{}\\t{}\\n\".format(uid, iid, test_dict[uid][iid]))\n",
    "\n",
    "t_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
